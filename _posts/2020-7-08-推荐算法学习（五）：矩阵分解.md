---
published: true
title: 推荐算法学习（五）：评分预测&矩阵分解
category: 推荐算法
tags: 
  - 评分预测
  - 矩阵分解
  - EVD
  - SVD
  - Surprise
  - Google Colab
layout: post
---

推荐系统的两大应用场景分别是**评分预测**（Rating Prediction）和**Top-N推荐**（Item Ranking）。其中评分预测主要用于评价网站，比如用户给自己看过的电影评多少分，或者用户给自己看过的书籍评价多少分，**矩阵分解**技术主要应用于评分预测问题；Top-N推荐常用于购物网站或拿不到显式评分的网站，通过用户的隐式反馈为用户提供一个可能感兴趣的Item列表，此排序任务需要排序模型进行建模。本文主要介绍如何利用矩阵分解来解决评分预测问题。

## 矩阵分解概述

在之前的文章中，我们介绍过常用的推荐算法，其中协同过滤是推荐系统的主流思想之一。协同过滤技术可划分为基于内存/邻域的协同过滤（Memory-based CF）与**基于模型的协同过滤**技术（Model-based CF）。这里基于模型的协同过滤技术中矩阵分解（Matrix Factorization，MF）技术最为普遍和流行，因为它的可扩展性好并且易于实现。 

矩阵分解是一种**隐语义模型**。隐语义模型是通过隐含特征（Latent Factor）联系用户兴趣和物品，基于用户行为的自动聚类。在评分预测问题中，收集到的用户行为评分数据可用一个评分矩阵表示，矩阵分解要做的是预测出评分矩阵中缺失的评分，使得预测评分能反映用户的喜欢程度，从而可以把预测评分最高的前K个电影推荐给用户。为了预测缺失的评分，矩阵分解学习User矩阵和Item矩阵，使得User矩阵*Item矩阵与评分矩阵中已知的评分差异最小，此时评分预测问题转化为了一个最优化问题，求解该最优化问题即可将评分矩阵分解成User矩阵和Item矩阵，从而可以计算出缺失的评分。

<img src="https://raw.githubusercontent.com/Alice1214/alice1214.github.io/master/_posts/image/推荐算法（五）/0.png" alt="0" style="zoom:70%;" />



以电影评分预测为例，用评分矩阵表示收集到的用户行为数据，12个用户，9部电影。

<img src="https://raw.githubusercontent.com/Alice1214/alice1214.github.io/master/_posts/image/推荐算法（五）/1.png" alt="0" style="zoom:60%;" />

观察下图User矩阵，可知用户的观影爱好体现在User向量上，观察Item矩阵，可知电影的风格体现在Item向量上，MF用user向量和item向量的内积去拟合评分矩阵中该user对该item的评分，内积的大小反映了user对item的喜欢程度，内积大喜欢程度高，反之喜欢程度低。这里“动作”、“动画”、“爱情”为隐含特征，隐含特征个数k越大，隐类别分得越细，计算量越大。

<img src="https://raw.githubusercontent.com/Alice1214/alice1214.github.io/master/_posts/image/推荐算法（五）/2.png" alt="0" style="zoom:150%;" />

把这两个矩阵相乘，就能预测每个用户对每部电影的预测评分了，评分值越大，表示用户喜欢该电影的可能性越大，该电影就越值得推荐给用户。

<img src="https://raw.githubusercontent.com/Alice1214/alice1214.github.io/master/_posts/image/推荐算法（五）/3.png" alt="0" style="zoom:60%;" />

接下来我们将介绍矩阵分解的目标函数及目标函数最优化问题的求解，并梳理推荐系统中出现过的经典的矩阵分解方法。

## 矩阵分解目标函数

$r_{ui}$表示用户$u$对Item $i$ 的评分，当 $r_{ui}$>0时，表示有评分，当$r_{ui}$=0时，表示没有评分；$x_u$表示用户$u$ 的向量，k维列向量；$y_i$表示Item $i$ 的向量，k维列向量；用户矩阵X，用户数为N，$X=[x_1,x_2,...,x_N]$；商品矩阵Y，商品数为M，$Y=[y_1,y_2,...,y_M]$.

用户向量与物品向量的内积，表示用户u 对物品i 的预测评分矩阵的目标函数：

<img src="https://raw.githubusercontent.com/Alice1214/alice1214.github.io/master/_posts/image/推荐算法（五）/4.png" alt="0" style="zoom:40%;" />

上式前半部分表示的是User矩阵*Item矩阵与评分矩阵中已知的评分差异，后半部分是L2正则项，保证数值计算稳定性，防止过拟合。

## 矩阵分解目标函数最优化问题求解

上述目标函数最优化问题的工程解法一般采用交替最小二乘法或随机梯度下降。

###　交替最小二乘法ALS

交替最小二乘法步骤：

* Step1，固定Y 优化X

<img src="https://raw.githubusercontent.com/Alice1214/alice1214.github.io/master/_posts/image/推荐算法（五）/5.png" alt="0" style="zoom:40%;" />

将目标函数转化为矩阵表达形式

<img src="https://raw.githubusercontent.com/Alice1214/alice1214.github.io/master/_posts/image/推荐算法（五）/6.png" alt="0" style="zoom:40%;" />

其中<img src="https://raw.githubusercontent.com/Alice1214/alice1214.github.io/master/_posts/image/推荐算法（五）/6.png" alt="0" style="zoom:40%;" />，表示用户u 对m个物品的评分；<img src="https://raw.githubusercontent.com/Alice1214/alice1214.github.io/master/_posts/image/推荐算法（五）/7.png" alt="0" style="zoom:40%;" />为m 个物品的向量。

* Step2，固定X 优化Y

* 重复Step1和2，直到X 和Y 收敛。每次固定一个矩阵，优化另一个矩阵，都是**最小二乘问题**



### 随机梯度下降SGD







## 特征分解EVD

用户在浏览网页过程中，并不都是按照跳转链接的方式来上网，假设还有一种可能是不论当前处于哪个页面，都有概率访问到其他任意的页面。根据这一假设场景，PageRank随机浏览模型引入**阻尼因子**d（通常取值为0.85），定义网页$u$的影响力为：

<img src="https://raw.githubusercontent.com/Alice1214/alice1214.github.io/master/_posts/image/推荐算法（四）/6.png" alt="0" style="zoom:27%;" />

## 奇异值分解SVD

PageRank不仅仅是一个算法，而是一种思想，TextRank算法、EdgeRank算法以及PersonalRank算法都是基于PageRank思想提出的。**TextRank**算法是一种用于文本的基于图的排序算法，可对文本进行关键词提取；**EdgeRank**算法是 Facebook 提出的对 fb 新鲜事排序的算法, 用于区别默认的按时间逆序的 timeline；**PersonalRank**算法是基于图的推荐算法，其将将用户行为转化为图模型，计算节点影响力，从而进行商品推荐。接下来介绍一下TextRank算法原理。

TextRank算法是根据词之间的共现关系构造网络，构造的网络中的边是无向有权边，权重为词共现的次数，基于PageRank原理计算节点影响力并进行排序，即可筛选出指定词性的关键词。其具体流程如下：

* Step1，进行分词和词性标注，将单词添加到图中
* Step2，出现在一个窗口中的词形成一条边
* Step3，基于PageRank原理进行迭代（20-30次）
* Step4，顶点（词）按照分数进行排序，可以筛选出指定词性的关键词

**TextRank不仅能提取关键词，还能生成摘要**。其流程是类似的，先将每个句子作为图中的节点，若两个句子相似，则节点之间存在一条无向有权边，这里句子的相似度等于同时出现在两个句子中的单词的个数除以句子中单词个数求对数之和，根据构造的图，即可计算句子（节点）的影响力，筛选出影响力大的句子从而生成摘要。

## ........

图是一个很重要的工具，其节点可以代表任何事物，因此能满足我们很多需求。如上文的节点影响力计算，社区发现以及最短路径等，都是基于图模型来处理的。本节将主要介绍一下常用的社区发现算法LPA。

现实中存在着各种**复杂网络**，如社交网络，交通网络，交易网络，食物链等。这些复杂网络的一个普遍特征就是社区结构，整个网络是由许多个社区组成的。**社区**的特点是同一社区内的节点与节点之间的连接很紧密，而社区与社区之间的连接很稀疏。**社区发现**是一种**聚类**算法，具体是指在图中确定n个社区，使得各社区的顶点集合的交集覆盖图的所有顶点。其中，若任意两个社区的顶点集合的交际均为空，则为**非重叠社区**，否则为**重叠社区**。常用的社区发现算法有**LPA**（基于标签传播的非重叠社区发现算法，**半监督学习**）和**COPRA**（基于LPA的扩展算法，用于重叠社区发现算法）。

LPA算法的步骤是：

* Step1，每个节点拥有独立的标签

* Step2，标签传播，节点向邻居节点传播自己的标签

* Step3，标签更新，每个节点的标签更新为邻居节点中出现次数最多的标签。如果存在多个选择，则随机选择一个

* Step4，如果节点更新后的标签发生了变化，则返回到Step2（激活状态），否则节点进入非激活状态，如果所有图中所有节点均为非激活状态，则标签更新结束。此时具有相同标签的节点属于同一个社区

LPA算法示意：

<img src="https://raw.githubusercontent.com/Alice1214/alice1214.github.io/master/_posts/image/推荐算法（四）/9.png" alt="0" style="zoom:70%;" />

* 使用加权和更新节点标签：圆形标签权重=0.2+0.8+0.1=1.1；三角形标签权重=0.4+0.6=1，所以节点标签更新为圆形

* 使用加权平均更新节点标签：圆形标签权重=(0.2+0.8+0.1)/3=0.367；三角形标签权重=(0.4+0.6)/2=0.5，节点标签更新为三角形

## 推荐系统工具：Surprise

**PageRank工具：**

* **igraph**：性能强大，可处理复杂网络问题，提供了Python, R, C语言接口，其效率比NetworkX高

* **NetworkX**：基于python的复杂网络库，对于Python使用者友好

**TextRank工具：**

* **jieba**：可进行中文分词；可使用TextRank/TF-IDF提取关键词，TF-IDF效果好于TextRank，因为考虑了IDF的情况，而TextRank倾向使用频繁词
* **textrank4zh**：可提取关键词，还可生成摘要；要依赖分词，分词提取结果及TextRank结果与jieba存在差异

## 【两章作业】实例1：使用PageRank分析希拉里邮箱丑闻关键人物

* **数据集**：<https://github.com/cystanford/PageRank>（希拉里邮件丑闻），包含了9306封邮件和513个人名

(1) Emails.csv：记录了所有公开邮件的内容，发送者和接受者的信息。

(2) Persons.csv：统计了邮件中所有人物的姓名及对应的ID。

(3) Aliases.csv：因为姓名存在别名的情况，为了将邮件中的人物进行统一，我们还需要用Aliases文件来查询别名和人物的对应关系。

* 将希拉里邮箱丑闻中人物看作节点，人物间发邮件关系看作边，设置边权重为发邮件的次数，如此生成的网络图如下<img src="https://raw.githubusercontent.com/Alice1214/alice1214.github.io/master/_posts/image/推荐算法（四）/7.png" alt="0" style="zoom:70%;" />

上图由于涉及人物众多、关系复杂，无法清晰的呈现邮箱丑闻中的关键人物。

* 计算每个节点（人物）的影响力（PR值），设置阈值，筛选出大于阈值的核心节点，绘制核心节点网络图如下<img src="https://raw.githubusercontent.com/Alice1214/alice1214.github.io/master/_posts/image/推荐算法（四）/8.png" alt="0" style="zoom:70%;" />

上图便能清晰明了的展现希拉里邮箱丑闻中关键人物及其之间的关系。

## 实例2：使用TextRank对新闻进行关键词提取及摘要输出

* **数据集**：news.txt

* **使用textrank4zh工具包，运用TextRank算法提取了新闻的关键词和关键句子（摘要）。**

>代码：<https://github.com/Alice1214/RS/blob/master/RS04/textrank_news.ipynb>

